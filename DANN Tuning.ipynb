{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bGLOLPY_fcw_"},"outputs":[],"source":["# !pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa1oenCm9wW_"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","from lxml import etree\n","import xml.etree.ElementTree as ET\n","import torch.backends.cudnn as cudnn\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","import re\n","\n","import torch\n","import torch.nn as nn\n","from transformers import BertModel, DistilBertModel, RobertaModel\n","from torch.autograd import Function\n","import torch.optim as optim\n","import csv\n","\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, RobertaTokenizer\n","import argparse\n","import math\n","import matplotlib.pyplot as plt\n","\n","import optuna\n","from optuna.trial import TrialState\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix, f1_score"]},{"cell_type":"markdown","metadata":{"id":"WI7cO-TxbBpK"},"source":["# **Seed**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lt_SWUHUbDV7"},"outputs":[],"source":["seed = 168\n","train_seed = 168"]},{"cell_type":"markdown","metadata":{"id":"_cfQq9uWYNbU"},"source":["# **Utility Functions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MG24y93vYSOR"},"outputs":[],"source":["class EarlyStopper:\n","    def __init__(self, patience=2, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.min_validation_loss = np.inf\n","\n","    def early_stop(self, validation_loss):\n","        if validation_loss < self.min_validation_loss:\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","        elif validation_loss > (self.min_validation_loss + self.min_delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False\n","\n","class InputFeatures(object):\n","    def __init__(self, input_ids, input_mask, label_id):\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.label_id = label_id\n","\n","def CSV2Array(path):\n","    data = pd.read_csv(path, encoding='latin')\n","    reviews, labels = data.reviews.values.tolist(), data.labels.values.tolist()\n","    return reviews, labels\n","\n","def make_cuda(tensor):\n","    if torch.cuda.is_available():\n","        tensor = tensor.cuda()\n","    return tensor\n","\n","def init_model(net, restore=None):\n","    # check if cuda is available\n","    if torch.cuda.is_available():\n","        cudnn.benchmark = True\n","        net.cuda()\n","    return net\n","\n","def text2features(reviews, labels, max_seq_length, tokenizer,\n","                                 cls_token='[CLS]', sep_token='[SEP]',\n","                                 pad_token=0):\n","    features = []\n","    for ex_index, (review, label) in enumerate(zip(reviews, labels)):\n","        tokens = tokenizer.tokenize(review)\n","        if len(tokens) > max_seq_length - 2:\n","            tokens = tokens[:(max_seq_length - 2)]\n","        tokens = [cls_token] + tokens + [sep_token]\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        input_mask = [1] * len(input_ids)\n","        padding_length = max_seq_length - len(input_ids)\n","        input_ids = input_ids + ([pad_token] * padding_length)\n","        input_mask = input_mask + ([0] * padding_length)\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","\n","        features.append(\n","            InputFeatures(input_ids=input_ids,\n","                          input_mask=input_mask,\n","                          label_id=label))\n","    return features\n","\n","def get_data_loader(features, batch_size, for_training = True):\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n","    dataset = TensorDataset(all_input_ids, all_input_mask, all_label_ids)\n","    if for_training:\n","        # For training\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    else:\n","        # For testing\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    return dataloader\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.device_count() > 0:\n","        torch.cuda.manual_seed_all(seed)"]},{"cell_type":"markdown","metadata":{"id":"GRwrCvsxYUN-"},"source":["# **Model Definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgwzXy8kYW5x"},"outputs":[],"source":["# Gradient Reversal Layer\n","class GradientReversalLayer(Function):\n","    @staticmethod\n","    def forward(ctx, x, alpha):\n","        ctx.alpha = alpha\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        output = grad_output.neg() * ctx.alpha\n","        return output, None\n","\n","# BERT encoder as Feature Extractor\n","class FeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(FeatureExtractor, self).__init__()\n","        self.encoder = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def forward(self, x, mask=None):\n","        outputs = self.encoder(x, attention_mask=mask)\n","        feat = outputs[1]\n","        return feat\n","\n","# Label Classifier for Sentiment Analysis\n","class LabelClassifier(nn.Module):\n","    def __init__(self, hidden_size=768, num_labels=3, dropout=0.5):\n","        super(LabelClassifier, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.classifier = nn.Linear(hidden_size, num_labels)\n","        self.apply(self.initialize_weights)\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        out = self.classifier(x)\n","        return out\n","\n","    def initialize_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=0.02)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","# Domain Clasifier as Discriminator\n","class DomainClassifier(nn.Module):\n","    def __init__(self, hidden_size=768, dropout=0.5):\n","        super(DomainClassifier, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.classifier = nn.Linear(hidden_size, 2)\n","        self.apply(self.initialize_weights)\n","\n","    def forward(self, x, alpha):\n","        x = self.dropout(x)\n","        x = GradientReversalLayer.apply(x, alpha)\n","        out = self.classifier(x)\n","        return out\n","\n","    def initialize_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=0.02)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()"]},{"cell_type":"markdown","metadata":{"id":"pJOUT2TNYd5p"},"source":["# **Evaluation looop**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXR_353YYg5v"},"outputs":[],"source":["def evaluate(encoder, classifier, data_loader, get_confusion_metric = False):\n","    encoder.eval()\n","    classifier.eval()\n","\n","    # Initialize loss and accuracy\n","    total_loss = 0\n","    total_acc = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    # Set loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Evaluate network\n","    for (reviews, mask, labels) in data_loader:\n","        reviews = make_cuda(reviews)\n","        mask = make_cuda(mask)\n","        labels = make_cuda(labels)\n","\n","        with torch.no_grad():\n","            feat = encoder(reviews, mask)\n","            preds = classifier(feat)\n","\n","        # Calculate loss\n","        total_loss += criterion(preds, labels).item()\n","\n","        # Get predictions and true labels\n","        pred_cls = preds.data.max(1)[1]  # Get the predicted class with highest score\n","        all_preds.extend(pred_cls.cpu().numpy())  # Store predicted labels\n","        all_labels.extend(labels.cpu().numpy())  # Store true labels\n","\n","        # Calculate accuracy\n","        total_acc += pred_cls.eq(labels.data).cpu().sum().item()\n","\n","    # Average loss and accuracy\n","    avg_loss = total_loss / len(data_loader)\n","    avg_acc = total_acc / len(data_loader.dataset)\n","\n","    if get_confusion_metric:\n","        # Calculate confusion matrix and F1 score\n","        conf_matrix = confusion_matrix(all_labels, all_preds)\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","        return avg_loss, avg_acc, conf_matrix, f1\n","    else:\n","        return avg_loss, avg_acc"]},{"cell_type":"markdown","metadata":{"id":"OmF9zAFAfcxV"},"source":["# **Optuna Hyperparameter Tuning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8mrEeuXfcxW","outputId":"98a09f0b-7b7f-4a96-d25f-bf1a8e00388e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-11-11 11:11:32,595] Using an existing study with name 'bert_hyperparameter_study' instead of creating a new one.\n"]},{"name":"stdout","output_type":"stream","text":["Fold: 1\n","Epoch: 0 Evaluation Accuracy: 0.834983498349835\n","Epoch: 1 Evaluation Accuracy: 0.8173817381738174\n","Epoch: 2 Evaluation Accuracy: 0.845984598459846\n","Early stopping triggered!\n","Fold: 2\n","Epoch: 0 Evaluation Accuracy: 0.8502202643171806\n","Epoch: 1 Evaluation Accuracy: 0.8072687224669604\n","Epoch: 2 Evaluation Accuracy: 0.816079295154185\n","Early stopping triggered!\n","Fold: 3\n","Epoch: 0 Evaluation Accuracy: 0.8491189427312775\n","Epoch: 1 Evaluation Accuracy: 0.8546255506607929\n","Epoch: 2 Evaluation Accuracy: 0.8414096916299559\n","Epoch: 3 Evaluation Accuracy: 0.8314977973568282\n","Early stopping triggered!\n","Fold: 4\n","Epoch: 0 Evaluation Accuracy: 0.8348017621145375\n","Epoch: 1 Evaluation Accuracy: 0.8689427312775331\n","Epoch: 2 Evaluation Accuracy: 0.8458149779735683\n","Epoch: 3 Evaluation Accuracy: 0.8381057268722467\n","Early stopping triggered!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-11-11 11:40:13,412] Trial 16 finished with value: 0.8329168544607766 and parameters: {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 0.001, 'weight_decay_constant': 0.001}. Best is trial 11 with value: 0.8458613207135691.\n"]}],"source":["def objective(trial):\n","    try:\n","        set_seed(train_seed) # Set Seed\n","        num_folds = 4  # Number of cross-validation folds\n","        avg_accuracy = 0\n","\n","        # Sample hyperparameters\n","        dropout = trial.suggest_categorical('dropout', [0.1, 0.2, 0.4])\n","        learning_rate = trial.suggest_categorical('learning_rate', [2e-5, 3e-5, 5e-5])\n","        alpha = trial.suggest_categorical('alpha', [1e-6, 1e-3, 1e-2])\n","        weight_decay_constant = trial.suggest_categorical('weight_decay_constant', [1e-4, 1e-3, 1e-2])\n","\n","        # Load BERT Tokenizer\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","        # Load reddit data as feature source data\n","        src_x, src_y = CSV2Array('./preprocessed_reddit.csv')\n","        src_x, src_test_x, src_y, src_test_y = train_test_split(src_x, src_y, test_size=0.2, stratify=src_y, random_state=seed)\n","        src_features = text2features(src_x, src_y, 75, tokenizer) # Source Features Train + Validation\n","        src_test_features = text2features(src_test_x, src_test_y, 75, tokenizer) # Source Feature Test\n","\n","        # Load financial data as feature target data\n","        tgt_x, tgt_y = CSV2Array('./preprocessed_financial.csv')\n","        tgt_train_x, tgt_test_x, tgt_train_y, tgt_test_y = train_test_split(tgt_x, tgt_y, test_size=0.25, stratify=tgt_y, random_state=seed)\n","        tgt_features = text2features(tgt_train_x, tgt_train_y, 75, tokenizer) # Target Features Train + Validation\n","\n","        # Cross-validation on target data\n","        kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","        f=0\n","        for train_index, val_index in kf.split(tgt_features):\n","            f+=1\n","            print(f\"Fold: {f}\")\n","            # Get target train and validation features\n","            tgt_train_features = [tgt_features[i] for i in train_index] # Target Feature Training\n","            tgt_val_features = [tgt_features[i] for i in val_index] # Target Feature Validation\n","\n","            # Get size of source training\n","            src_size = len(src_features)\n","            tgt_size = len(tgt_train_features)\n","\n","            # Create data loaders\n","            batch_size = 128\n","            tgt_data_loader_batch_size = math.ceil((tgt_size/src_size) * batch_size)\n","            src_data_loader = get_data_loader(src_features, batch_size, for_training=True) # Src Data Loader Training\n","            tgt_data_loader = get_data_loader(tgt_train_features, tgt_data_loader_batch_size, for_training=True) # Target Data Loader Training\n","            tgt_data_loader_val = get_data_loader(tgt_val_features, batch_size, for_training=False) # Target Data Loader Validation\n","\n","            # Load and initialize models\n","            encoder = FeatureExtractor()\n","            cls_classifier = LabelClassifier(dropout=dropout)\n","            dom_classifier = DomainClassifier(dropout=dropout)\n","            encoder = init_model(encoder)\n","            cls_classifier = init_model(cls_classifier)\n","            dom_classifier = init_model(dom_classifier)\n","\n","            # Setup 0ptimizer\n","            optimizer = optim.AdamW(list(encoder.parameters()) + list(cls_classifier.parameters()) + list(dom_classifier.parameters()),\n","                lr=learning_rate,\n","                weight_decay=weight_decay_constant\n","            )\n","\n","            # Initializations\n","            num_epochs=10\n","\n","            # Setup criterion\n","            CELoss = nn.CrossEntropyLoss()\n","\n","            # Initialize Early Stopper\n","            early_stopper = EarlyStopper()\n","\n","            for epoch in range(num_epochs):\n","                # Set Train State\n","                encoder.train()\n","                cls_classifier.train()\n","                dom_classifier.train()\n","\n","                data_zip = enumerate(zip(src_data_loader, tgt_data_loader))\n","                for step, ((src_reviews, src_mask, src_labels), (tgt_reviews, tgt_mask, tgt_labels)) in data_zip:\n","                    src_reviews = make_cuda(src_reviews)\n","                    src_mask = make_cuda(src_mask)\n","                    src_labels = make_cuda(src_labels)\n","                    tgt_reviews = make_cuda(tgt_reviews)\n","                    tgt_mask = make_cuda(tgt_mask)\n","                    tgt_labels = make_cuda(tgt_labels)\n","\n","                    # Extract and concat features\n","                    src_feat = encoder(src_reviews, src_mask)\n","                    tgt_feat = encoder(tgt_reviews, tgt_mask)\n","                    feat_concat = torch.cat((src_feat, tgt_feat), 0)\n","                    src_preds = cls_classifier(src_feat)\n","                    dom_preds = dom_classifier(feat_concat, alpha=alpha)\n","\n","                    # Prepare domain label\n","                    optimizer.zero_grad()\n","                    label_src = make_cuda(torch.ones(src_feat.size(0))) # 1 is src\n","                    label_tgt = make_cuda(torch.zeros(tgt_feat.size(0))) # 0 is target domain\n","                    label_concat = torch.cat((label_src, label_tgt), 0).long()\n","                    loss_cls = CELoss(src_preds, src_labels)\n","                    loss_dom = CELoss(dom_preds, label_concat)\n","                    loss_tgt_cls = CELoss(cls_classifier(tgt_feat), tgt_labels)\n","                    loss = loss_cls + loss_dom + loss_tgt_cls\n","\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                # Evaluate Current Fold Current Epoch for Target\n","                eval_tgt_loss, eval_tgt_acc = evaluate(encoder, cls_classifier, tgt_data_loader_val)\n","                print(f\"Epoch: {epoch} Evaluation Accuracy: {eval_tgt_acc}\")\n","\n","                # Early stopping\n","                if early_stopper.early_stop(eval_tgt_loss):\n","                    print(\"Early stopping triggered!\")\n","                    break\n","\n","            # Accumulate maximum accuracy over folds\n","            avg_accuracy += eval_tgt_acc\n","\n","        # Calculate average accuracy over all folds\n","        avg_accuracy /= num_folds\n","\n","        # Return average validation accuracy for optimization\n","        return avg_accuracy\n","\n","    except Exception as e:\n","        print(e)\n","        return 0.0\n","\n","\n","# Define the SQLite database URL\n","db_url = \"sqlite:///optuna_study.db\"\n","\n","# Create a study and store it in an SQLite database\n","study = optuna.create_study(direction=\"maximize\", storage=db_url, study_name=\"bert_hyperparameter_study\", load_if_exists=True)\n","study.optimize(objective, n_trials=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLRuYGSHfcxY","outputId":"39710da5-0207-4f58-9118-835eca91d34d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 0: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 1e-06, 'weight_decay_constant': 0.01}, Value = 0.8362235452620151\n","Trial 1: State = 1, Params = {'dropout': 0.4, 'learning_rate': 5e-05, 'alpha': 0.01, 'weight_decay_constant': 0.0001}, Value = 0.8370510509200699\n","Trial 2: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 1e-06, 'weight_decay_constant': 0.0001}, Value = 0.8453966453914115\n","Trial 3: State = 1, Params = {'dropout': 0.1, 'learning_rate': 5e-05, 'alpha': 1e-06, 'weight_decay_constant': 0.01}, Value = 0.8304473619168085\n","Trial 4: State = 1, Params = {'dropout': 0.4, 'learning_rate': 5e-05, 'alpha': 1e-06, 'weight_decay_constant': 0.001}, Value = 0.8210806763495733\n","Trial 5: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 1e-06, 'weight_decay_constant': 0.01}, Value = 0.8362235452620151\n","Trial 6: State = 1, Params = {'dropout': 0.4, 'learning_rate': 3e-05, 'alpha': 1e-06, 'weight_decay_constant': 0.001}, Value = 0.8414578517323098\n","Trial 7: State = 1, Params = {'dropout': 0.2, 'learning_rate': 3e-05, 'alpha': 1e-06, 'weight_decay_constant': 0.0001}, Value = 0.8342989585302143\n","Trial 8: State = 1, Params = {'dropout': 0.2, 'learning_rate': 5e-05, 'alpha': 0.001, 'weight_decay_constant': 0.01}, Value = 0.8301677910081757\n","Trial 9: State = 1, Params = {'dropout': 0.1, 'learning_rate': 5e-05, 'alpha': 0.01, 'weight_decay_constant': 0.0001}, Value = 0.8370510509200699\n","Trial 10: State = 1, Params = {'dropout': 0.2, 'learning_rate': 2e-05, 'alpha': 1e-06, 'weight_decay_constant': 0.0001}, Value = 0.8329168544607766\n","Trial 11: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 0.001, 'weight_decay_constant': 0.0001}, Value = 0.8458613207135691\n","Trial 12: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 0.001, 'weight_decay_constant': 0.0001}, Value = 0.8458613207135691\n","Trial 13: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 0.001, 'weight_decay_constant': 0.0001}, Value = 0.8458613207135691\n","Trial 14: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 0.001, 'weight_decay_constant': 0.0001}, Value = 0.8458613207135691\n","Trial 15: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 0.001, 'weight_decay_constant': 0.0001}, Value = 0.8458613207135691\n","Trial 16: State = 1, Params = {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 0.001, 'weight_decay_constant': 0.001}, Value = 0.8329168544607766\n"]}],"source":["for trial in study.trials:\n","    print(f\"Trial {trial.number}: State = {trial.state}, Params = {trial.params}, Value = {trial.value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5swKjfLfcxZ","outputId":"870ef9fe-ee0d-4988-ac67-b24fa9c1b13f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'dropout': 0.1, 'learning_rate': 2e-05, 'alpha': 0.001, 'weight_decay_constant': 0.0001}\n"]}],"source":["print(\"Best hyperparameters:\", study.best_trial.params)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}